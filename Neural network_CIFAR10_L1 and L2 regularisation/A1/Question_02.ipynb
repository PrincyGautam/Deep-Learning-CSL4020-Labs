{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVh1eV-OYN59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# Here, upload the zipped file of GurNum folder\n",
        "import zipfile\n",
        "import io\n",
        "data =\n",
        "zipfile.ZipFile(io.BytesIO(uploaded['GurNum-20230130T135920Z-001.zip']),\n",
        "'r')\n",
        "data.extractall()\n",
        "data.printdir()\n",
        "train_data_dir = 'GurNum/train'\n",
        "test_data_dir = 'GurNum/val'\n",
        "import os\n",
        "target_names = [item for item in os.listdir(train_data_dir) if\n",
        "os.path.isdir(os.path.join(train_data_dir, item))]\n",
        "nb_train_samples = sum([len(files) for _, _, files in\n",
        "os.walk(train_data_dir)])\n",
        "nb_test_samples = sum([len(files) for _, _, files in\n",
        "os.walk(test_data_dir)])\n",
        "total_nb_samples = nb_train_samples + nb_test_samples\n",
        "nb_classes = len(target_names)\n",
        "epochs = 2\n",
        "batch_size = 4\n",
        "print('Training a CNN Multi-Classifier Model ......')\n",
        "print('\\n - names of classes: ', target_names, '\\n - # of classes: ',\n",
        "nb_classes)\n",
        "print(' - # of trained samples: ', nb_train_samples,\n",
        "'\\n - # of test samples: ', nb_test_samples,\n",
        "'\\n - total # of samples: ', total_nb_samples,\n",
        "'\\n - train ratio:', round(nb_train_samples/total_nb_samples*100,\n",
        "2),\n",
        "'\\n - test ratio:', round(nb_test_samples/total_nb_samples*100, 2),\n",
        "' %', '\\n - # of epochs: ', epochs, '\\n - batch size: ', batch_size)\n",
        "# Define the data transform to be applied on the dataset\n",
        "data_transform = transforms.Compose([\n",
        "transforms.Resize((32, 32)),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Load the training and validation datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_data_dir,\n",
        "transform=data_transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder(test_data_dir,\n",
        "transform=data_transform)\n",
        "# Define the data loaders for training and validation\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
        "shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32,\n",
        "shuffle=True)\n",
        "# Define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "def __init__(self):\n",
        "super(Net, self).__init__()\n",
        "self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "self.pool = nn.MaxPool2d(2, 2)\n",
        "self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "self.fc2 = nn.Linear(120, 84)\n",
        "self.fc3 = nn.Linear(84, nb_classes)\n",
        "def forward(self, x):\n",
        "x = self.pool(torch.relu(self.conv1(x)))\n",
        "x = self.pool(torch.relu(self.conv2(x)))\n",
        "x = x.view(-1, 16 * 5 * 5)\n",
        "x = torch.relu(self.fc1(x))\n",
        "x = nn.Dropout(p=0.5)(x) # Add dropout layer after fc1 layer\n",
        "x = torch.relu(self.fc2(x))\n",
        "x = nn.Dropout(p=0.5)(x) # Add dropout layer after fc2 layer\n",
        "x = self.fc3(x)\n",
        "return x\n",
        "def gradient_checking(net, inputs, labels, criterion):\n",
        "epsilon = 1e-6\n",
        "parameters = list(net.parameters())\n",
        "grads = [None] * len(parameters)\n",
        "for i, p in enumerate(parameters):\n",
        "p_tensor = p.data.clone()\n",
        "original = p_tensor.detach().numpy()[0]\n",
        "p_tensor = torch.tensor(original + epsilon,\n",
        "requires_grad=True)\n",
        "net.zero_grad()\n",
        "output = net(inputs)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "grads[i] = parameters[i].grad.detach().numpy()[0]\n",
        "p_tensor = torch.tensor(original, requires_grad=True)\n",
        "net.zero_grad()\n",
        "output = net(inputs)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "grads[i] -= parameters[i].grad.detach().numpy()[0]\n",
        "grads[i] /= epsilon\n",
        "return grads\n",
        "# Initialize the CNN\n",
        "net = Net()\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# Train the CNN and calculate val dataset accuracy\n",
        "for epoch in range(10): # loop over the dataset multiple times\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "inputs, labels = data\n",
        "optimizer.zero_grad()\n",
        "outputs = net(inputs)\n",
        "loss = criterion(outputs, labels)\n",
        "# Call the gradient checking function\n",
        "net.gradient_checking(inputs, labels, criterion)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "running_loss += loss.item()\n",
        "print('Epoch %d Train loss: %.3f' % (epoch + 1, running_loss / (i +\n",
        "1)))\n",
        "# Evaluate the model on the validation dataset\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "total = 0\n",
        "for data in val_loader:\n",
        "images, labels = data\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "total += labels.size(0)\n",
        "correct += (predicted == labels).sum().item()\n",
        "print('Accuracy of the network on the validation images: %d %%' %\n",
        "(100 * correct / total))\n",
        "# Define the custom L1 regularization term\n",
        "def l1_regularization(model):\n",
        "l1_reg = 0\n",
        "for param in model.parameters():\n",
        "l1_reg += torch.norm(param, p=1)\n",
        "return l1_reg\n",
        "# Define the custom L2 regularization term\n",
        "def l2_regularization(model):\n",
        "l2_reg = 0\n",
        "for param in model.parameters():\n",
        "l2_reg += torch.norm(param, p=2)\n",
        "return l2_reg\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the regularization strength\n",
        "reg_strength = 0.1\n",
        "# Define the optimizer with L1 regularization\n",
        "optimizer_L1 = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,\n",
        "weight_decay=reg_strength)\n",
        "# Train the CNN with L1 regularization\n",
        "for epoch in range(10): # loop over the dataset multiple times\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "inputs, labels = data\n",
        "optimizer_L1.zero_grad()\n",
        "outputs = net(inputs)\n",
        "loss = criterion(outputs, labels) + reg_strength *\n",
        "l1_regularization(net)\n",
        "loss.backward()\n",
        "optimizer_L1.step()\n",
        "running_loss += loss.item()\n",
        "print('Epoch %d loss with L1 regularization: %.3f' % (epoch + 1,\n",
        "running_loss / (i + 1)))\n",
        "# Define the optimizer with L2 regularization\n",
        "optimizer_L2 = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,\n",
        "weight_decay=reg_strength)\n",
        "# Train the CNN with L2 regularization\n",
        "for epoch in range(10): # loop over the dataset multiple times\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "inputs, labels = data\n",
        "optimizer_L2.zero_grad()\n",
        "outputs = net(inputs)\n",
        "loss = criterion(outputs, labels) + reg_strength *\n",
        "l2_regularization(net)\n",
        "loss.backward()\n",
        "optimizer_L2.step()\n",
        "running_loss += loss.item()\n",
        "print('Epoch %d loss with L2 regularization: %.3f' % (epoch + 1,\n",
        "running_loss / (i + 1)))\n",
        "#torch.save(net.state_dict(), 'model.pt')"
      ]
    }
  ]
}